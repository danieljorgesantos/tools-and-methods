# Large Scale Offline Reinforcement Learning

Based on StarCraft II Unplugged: Large Scale Offline Reinforcement Learning

https://openreview.net/pdf?id=Np8Pumfoty

### Offline training 

In the offline setting, agents learn from a fixed
dataset previously logged by humans or other agents.

 In the offline setting, the agent does not interact with the MDP
but learns only from a dataset D containing sequences (st, at, rt+1)
N
t=0. The dataset D is assumed
to have been generated by following an unknown behaviour policy µ, a distribution over actions
conditioned on the state: µ(a|s).

The goal of offline RL is to find a policy ⇡ that maximizes the expected discounted return.

### Action space

The action space can vary depending on the specific problem and the nature of the environment. It can be discrete, where there is a finite set of distinct actions that the agent can choose from. For example, in a game like Tic-Tac-Toe, the action space consists of all the possible board positions where the agent can place its mark.

Alternatively, the action space can be continuous, which means it encompasses a range of possible actions. In this case, the agent can select actions from a continuous set of values. For example, in a robotic control task, the action space might include the torques or velocities that the robot can apply to its joints.


###  Stochastic environments

"stochasticity" refers to the presence of randomness or uncertainty in the environment or system being discussed. It indicates that certain elements or factors within the environment or system are subject to random variations or probabilities, leading to unpredictable outcomes. Stochasticity can arise from different sources, such as the behavior of an unknown opponent policy, random movement or spawn points of units, random delays in executing commands, or sticky actions in the case of Atari environments. Overall, stochasticity introduces an element of chance or variability that affects the dynamics and outcomes of the system or environment.

### Partial Observability. 

StarCraft II is an imperfect information game. Players only have information
about opponent units that are within the field of view of the player’s own units. As a result, players
need to scout, i.e. send their units around the map to gather information about the current state of the
game, and may need it at a later point in the game. In comparison, a memory of the 3 most recent
frames is usually considered sufficient for Atari.

##  Methodology

### Training

#### offline dataset aproach

Algorithms may only
use data from the dataset 

#### online policy evaluation

we allow for online policy evaluation, i.e.
policies can be run in the environment to measure how well they perform. This evaluation may be
useful for hyperparameter tuning.

